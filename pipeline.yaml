# PIPELINE DEFINITION
# Name: xgboost-pipeline-with-deployment-v2
# Description: Full XGBoost MLOps pipeline:
#              1. Load dataset
#              2. Train model
#              3. Evaluate model
#              4. Conditionally deploy model if score >= threshold
# Outputs:
#    eval-model-metrics: system.ClassificationMetrics
#    eval-model-smetrics: system.Metrics
components:
  comp-condition-1:
    dag:
      tasks:
        deploy-xgboost-model:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-deploy-xgboost-model
          inputs:
            artifacts:
              model:
                componentInputArtifact: pipelinechannel--train-model-model_artifact
            parameters:
              project_id:
                runtimeValue:
                  constant: magnetic-guild-473821-t7
          taskInfo:
            name: deploy-xgboost-model
    inputDefinitions:
      artifacts:
        pipelinechannel--train-model-model_artifact:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        pipelinechannel--eval-model-deploy:
          parameterType: STRING
  comp-deploy-xgboost-model:
    executorLabel: exec-deploy-xgboost-model
    inputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
          description: 'Input[Model]

            The trained model to deploy.'
      parameters:
        project_id:
          description: 'str

            The Google Cloud project ID.'
          parameterType: STRING
    outputDefinitions:
      artifacts:
        vertex_endpoint:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        vertex_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-eval-model:
    executorLabel: exec-eval-model
    inputDefinitions:
      artifacts:
        test_set:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
          description: Input[Dataset] - CSV with a target column 'will_buy_on_return_visit'.
        xgb_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
          description: Input[Model] - Trained model artifact saved as 'model.joblib'
            in GCS.
      parameters:
        bucket_name:
          description: str - GCS bucket name (no gs:// prefix).
          parameterType: STRING
        score_threshold:
          defaultValue: 0.8
          description: float - minimum accuracy required to deploy.
          isOptional: true
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      artifacts:
        metrics:
          artifactType:
            schemaTitle: system.ClassificationMetrics
            schemaVersion: 0.0.1
        smetrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
      parameters:
        deploy:
          parameterType: STRING
  comp-get-data:
    executorLabel: exec-get-data
    inputDefinitions:
      parameters:
        project_id:
          description: str, Google Cloud project ID
          parameterType: STRING
    outputDefinitions:
      artifacts:
        dataset_test:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        dataset_train:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-train-model:
    executorLabel: exec-train-model
    inputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
          description: 'Input[Dataset]

            The training dataset as a Kubeflow component input.'
    outputDefinitions:
      artifacts:
        model_artifact:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
defaultPipelineRoot: gs://magnetic-guild-473821-t7-bucket/pipeline_root/
deploymentSpec:
  executors:
    exec-deploy-xgboost-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - deploy_xgboost_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform==1.25.0'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef deploy_xgboost_model(\n    model: Input[Model],\n    project_id:\
          \ str,\n    vertex_endpoint: Output[Artifact],\n    vertex_model: Output[Model],\n\
          ) -> None:\n    \"\"\"\n    Deploys an XGBoost model to Vertex AI Endpoint.\n\
          \n    Args:\n        model: Input[Model]\n            The trained model\
          \ to deploy.\n        project_id: str\n            The Google Cloud project\
          \ ID.\n        vertex_endpoint: Output[Artifact]\n            Represents\
          \ the deployed Vertex AI Endpoint.\n        vertex_model: Output[Model]\n\
          \            Represents the deployed Vertex AI Model.\n\n    Returns:\n\
          \        None\n    \"\"\"\n    from google.cloud import aiplatform\n\n \
          \   # --- Initialize Vertex AI with the project ---\n    aiplatform.init(project=project_id)\n\
          \n    # --- Upload the trained model to Vertex AI ---\n    deployed_model\
          \ = aiplatform.Model.upload(\n        display_name=\"xgb-classification\"\
          ,\n        artifact_uri=model.uri,\n        serving_container_image_uri=(\n\
          \            \"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-6:latest\"\
          \n        ),\n    )\n\n    # --- Deploy model to an endpoint (with machine\
          \ type) ---\n    endpoint = deployed_model.deploy(machine_type=\"n1-standard-4\"\
          )\n\n    # --- Save outputs so other pipeline steps can use them ---\n \
          \   vertex_endpoint.uri = endpoint.resource_name\n    vertex_model.uri =\
          \ deployed_model.resource_name\n\n"
        image: python:3.8
    exec-eval-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - eval_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.6.2'\
          \ 'pandas==1.3.5' 'joblib==1.1.0' 'scikit-learn==1.0.2' 'google-cloud-storage==2.13.0'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef eval_model(\n    test_set: Input[Dataset],\n    xgb_model: Input[Model],\n\
          \    metrics: Output[ClassificationMetrics],\n    smetrics: Output[Metrics],\n\
          \    bucket_name: str,\n    score_threshold: float = 0.8,\n) -> NamedTuple(\"\
          Outputs\", [(\"deploy\", str)]):\n    \"\"\"\n    Evaluates an XGBoost model\
          \ on a test dataset, logs metrics, and decides whether to deploy.\n\n  \
          \  Args:\n        test_set: Input[Dataset] - CSV with a target column 'will_buy_on_return_visit'.\n\
          \        xgb_model: Input[Model] - Trained model artifact saved as 'model.joblib'\
          \ in GCS.\n        metrics: Output[ClassificationMetrics] - For ROC curve\
          \ and confusion matrix.\n        smetrics: Output[Metrics] - For scalar\
          \ metrics like accuracy.\n        bucket_name: str - GCS bucket name (no\
          \ gs:// prefix).\n        score_threshold: float - minimum accuracy required\
          \ to deploy.\n\n    Returns:\n        NamedTuple(\"Outputs\", [(\"deploy\"\
          , str)]) - \"true\" or \"false\".\n    \"\"\"\n    # --- Imports inside\
          \ component image ---\n    from google.cloud import storage\n    import\
          \ joblib\n    import pandas as pd\n    from sklearn.metrics import roc_curve,\
          \ confusion_matrix\n\n    # ---------- 1) Load test data ----------\n  \
          \  data = pd.read_csv(test_set.path)\n    X = data.drop(columns=[\"will_buy_on_return_visit\"\
          ])\n    y = data[\"will_buy_on_return_visit\"]\n\n    # ---------- 2) Load\
          \ trained model from GCS ----------\n    client = storage.Client()\n   \
          \ bucket = client.bucket(bucket_name)\n\n    # xgb_model.uri is like: gs://<bucket>/<path-to-artifact-dir>\n\
          \    gs_uri = xgb_model.uri  # e.g., \"gs://my-bucket/pipelines/.../model\"\
          \n    prefix = f\"gs://{bucket_name}/\"\n    # Path inside the bucket to\
          \ the model directory\n    blob_dir = gs_uri[len(prefix):] if gs_uri.startswith(prefix)\
          \ else gs_uri\n    smetrics.log_metric(\"blob_path\", str(blob_dir))\n\n\
          \    # Model file name created by training step\n    model_blob = bucket.blob(f\"\
          {blob_dir}/model.joblib\")\n    with model_blob.open(\"rb\") as f:\n   \
          \     model = joblib.load(f)\n\n    # ---------- 3) Evaluate ----------\n\
          \    # Probabilities for positive class\n    y_scores = model.predict_proba(X)[:,\
          \ 1]\n    # Class predictions\n    y_pred = model.predict(X)\n    # Simple\
          \ accuracy (demo)\n    score = model.score(X, y)\n\n    # ---------- 4)\
          \ Log metrics to KFP UI ----------\n    fpr, tpr, thresholds = roc_curve(y.to_numpy(),\
          \ y_scores, pos_label=1)\n    metrics.log_roc_curve(fpr.tolist(), tpr.tolist(),\
          \ thresholds.tolist())\n\n    # Confusion matrix: ensure label order matches\
          \ categories\n    labels = [0, 1]\n    cm = confusion_matrix(y, y_pred,\
          \ labels=labels)\n    metrics.log_confusion_matrix([\"False\", \"True\"\
          ], cm.tolist())\n\n\n\n    smetrics.log_metric(\"accuracy\", float(score))\n\
          \n    # ---------- 5) Deployment decision ----------\n    deploy = \"true\"\
          \ if score >= score_threshold else \"false\"\n\n    # ---------- 6) Write\
          \ back artifact metadata ----------\n    xgb_model.metadata[\"test_score\"\
          ] = float(score)\n\n    # ---------- 7) Return flag ----------\n    Outputs\
          \ = NamedTuple(\"Outputs\", [(\"deploy\", str)])\n    return Outputs(deploy)\n\
          \n"
        image: python:3.8
    exec-get-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - get_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas==1.3.4'\
          \ 'scikit-learn==1.0.1' 'google-cloud-bigquery==3.13.0' 'db-dtypes==1.1.1'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef get_data(\n    project_id: str,\n    dataset_train: Output[Dataset],\n\
          \    dataset_test: Output[Dataset],\n) -> None:\n    \"\"\"\n    Loads data\
          \ from BigQuery, splits it into training and test sets,\n    and saves them\
          \ as CSV files.\n\n    Args:\n        project_id: str, Google Cloud project\
          \ ID\n        dataset_train: Output[Dataset] for the training set\n    \
          \    dataset_test: Output[Dataset] for the test set\n    \"\"\"\n\n    #\
          \ Imports inside component so they're installed in the container\n    from\
          \ sklearn.model_selection import train_test_split\n    import pandas as\
          \ pd\n    from google.cloud import bigquery\n\n    # Construct a BigQuery\
          \ client object\n    client = bigquery.Client(project=project_id)\n    job_config\
          \ = bigquery.QueryJobConfig()\n\n    # BigQuery SQL query\n    query = \"\
          \"\"\n    SELECT\n      * EXCEPT(fullVisitorId)\n    FROM\n    (\n     \
          \ SELECT\n        fullVisitorId,\n        IFNULL(totals.bounces, 0) AS bounces,\n\
          \        IFNULL(totals.timeOnSite, 0) AS time_on_site\n      FROM `data-to-insights.ecommerce.web_analytics`\n\
          \      WHERE totals.newVisits = 1\n        AND date BETWEEN '20160801' AND\
          \ '20170430'  # train on first 9 months\n    )\n    JOIN\n    (\n      SELECT\n\
          \        fullVisitorId,\n        IF(COUNTIF(totals.transactions > 0 AND\
          \ totals.newVisits IS NULL) > 0, 1, 0) \n          AS will_buy_on_return_visit\n\
          \      FROM `data-to-insights.ecommerce.web_analytics`\n      GROUP BY fullVisitorId\n\
          \    )\n    USING (fullVisitorId)\n    LIMIT 10000\n    \"\"\"\n\n    #\
          \ Run query and load results into DataFrame\n    query_job = client.query(query,\
          \ job_config=job_config)\n    df = query_job.to_dataframe()\n\n    # Split\
          \ dataset into train/test sets\n    train, test = train_test_split(df, test_size=0.3,\
          \ random_state=42)\n\n    # Save to pipeline outputs\n    train.to_csv(dataset_train.path,\
          \ index=False)\n    test.to_csv(dataset_test.path, index=False)\n\n"
        image: python:3.8
    exec-train-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.6.2'\
          \ 'pandas==1.3.5' 'joblib==1.1.0' 'scikit-learn==1.0.2' && \"$0\" \"$@\"\
          \n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_model(\n    dataset: Input[Dataset],\n    model_artifact:\
          \ Output[Model],\n) -> None:\n    \"\"\"\n    Trains an XGBoost classifier\
          \ on a given dataset and saves the model artifact.\n\n    Args:\n      \
          \  dataset: Input[Dataset]\n            The training dataset as a Kubeflow\
          \ component input.\n        model_artifact: Output[Model]\n            A\
          \ Kubeflow component output for saving the trained model.\n\n    Returns:\n\
          \        None\n        This function doesn't return a value; its primary\
          \ purpose is to produce a model artifact.\n    \"\"\"\n    import os\n \
          \   import joblib\n    import pandas as pd\n    from xgboost import XGBClassifier\n\
          \n    # Load Training Data\n    data = pd.read_csv(dataset.path)\n\n   \
          \ # Train XGBoost Model\n    model = XGBClassifier(objective=\"binary:logistic\"\
          )\n    model.fit(\n        data.drop(columns=[\"will_buy_on_return_visit\"\
          ]),\n        data.will_buy_on_return_visit,\n    )\n\n    # Evaluate and\
          \ Log Metrics\n    score = model.score(\n        data.drop(columns=[\"will_buy_on_return_visit\"\
          ]),\n        data.will_buy_on_return_visit,\n    )\n\n    # Save the Model\
          \ Artifact\n    os.makedirs(model_artifact.path, exist_ok=True)\n    joblib.dump(model,\
          \ os.path.join(model_artifact.path, \"model.joblib\"))\n\n    # Metadata\
          \ for the Artifact\n    model_artifact.metadata[\"train_score\"] = float(score)\n\
          \    model_artifact.metadata[\"framework\"] = \"XGBoost\"\n\n"
        image: python:3.8
pipelineInfo:
  description: 'Full XGBoost MLOps pipeline:

    1. Load dataset

    2. Train model

    3. Evaluate model

    4. Conditionally deploy model if score >= threshold'
  name: xgboost-pipeline-with-deployment-v2
root:
  dag:
    outputs:
      artifacts:
        eval-model-metrics:
          artifactSelectors:
          - outputArtifactKey: metrics
            producerSubtask: eval-model
        eval-model-smetrics:
          artifactSelectors:
          - outputArtifactKey: smetrics
            producerSubtask: eval-model
    tasks:
      condition-1:
        componentRef:
          name: comp-condition-1
        dependentTasks:
        - eval-model
        - train-model
        inputs:
          artifacts:
            pipelinechannel--train-model-model_artifact:
              taskOutputArtifact:
                outputArtifactKey: model_artifact
                producerTask: train-model
          parameters:
            pipelinechannel--eval-model-deploy:
              taskOutputParameter:
                outputParameterKey: deploy
                producerTask: eval-model
        taskInfo:
          name: deploy
        triggerPolicy:
          condition: inputs.parameter_values['pipelinechannel--eval-model-deploy']
            == 'true'
      eval-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-eval-model
        dependentTasks:
        - get-data
        - train-model
        inputs:
          artifacts:
            test_set:
              taskOutputArtifact:
                outputArtifactKey: dataset_test
                producerTask: get-data
            xgb_model:
              taskOutputArtifact:
                outputArtifactKey: model_artifact
                producerTask: train-model
          parameters:
            bucket_name:
              runtimeValue:
                constant: magnetic-guild-473821-t7-bucket
        taskInfo:
          name: eval-model
      get-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-get-data
        inputs:
          parameters:
            project_id:
              runtimeValue:
                constant: magnetic-guild-473821-t7
        taskInfo:
          name: get-data
      train-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-model
        dependentTasks:
        - get-data
        inputs:
          artifacts:
            dataset:
              taskOutputArtifact:
                outputArtifactKey: dataset_train
                producerTask: get-data
        taskInfo:
          name: train-model
  outputDefinitions:
    artifacts:
      eval-model-metrics:
        artifactType:
          schemaTitle: system.ClassificationMetrics
          schemaVersion: 0.0.1
      eval-model-smetrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
schemaVersion: 2.1.0
sdkVersion: kfp-2.7.0
