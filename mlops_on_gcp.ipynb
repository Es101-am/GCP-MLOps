{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e7ac9f1-a7ab-42fb-9ab8-fdfd48c56915",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 2.14.3\n",
      "google-cloud-aiplatform==1.113.0\n"
     ]
    }
   ],
   "source": [
    "# ! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! pip3 freeze | grep aiplatform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff1d288-543a-4624-bc41-bdba1b8b0619",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: kfp 2.7.0\n",
      "Uninstalling kfp-2.7.0:\n",
      "  Successfully uninstalled kfp-2.7.0\n",
      "Found existing installation: kfp-pipeline-spec 0.3.0\n",
      "Uninstalling kfp-pipeline-spec-0.3.0:\n",
      "  Successfully uninstalled kfp-pipeline-spec-0.3.0\n",
      "Found existing installation: kfp-server-api 2.0.5\n",
      "Uninstalling kfp-server-api-2.0.5:\n",
      "  Successfully uninstalled kfp-server-api-2.0.5\n",
      "KFP: 2.7.0 Path: /opt/conda/lib/python3.10/site-packages/kfp/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import sys, importlib\n",
    "\n",
    "# uninstall + install with the SAME python the notebook uses\n",
    "!{sys.executable} -m pip uninstall -y kfp kfp-pipeline-spec kfp-server-api\n",
    "!{sys.executable} -m pip install -q \"kfp==2.7.0\"\n",
    "\n",
    "# verify the version and where it loads from\n",
    "import kfp\n",
    "print(\"KFP:\", kfp.__version__, \"Path:\", kfp.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa8b18f7-1b09-4458-9e85-0c6bc9999a50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "import kfp\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import Artifact, Dataset, Input, Metrics, Model, Output, component, ClassificationMetrics\n",
    "\n",
    "from collections import namedtuple\n",
    "from typing import NamedTuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b3112a2-52e9-4c21-beb3-ca9af2c6321b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = 'magnetic-guild-473821-t7'   # replace with your own GCP project ID\n",
    "REGION = 'us-central1'                 # region where Vertex AI runs\n",
    "EXPERIMENT = 'vertex-pipelines'        # experiment name (for ML Metadata tracking)\n",
    "SERIES = 'dev'                         # series label, can be used for grouping runs\n",
    "\n",
    "# gcs bucket\n",
    "GCS_BUCKET = PROJECT_ID\n",
    "BUCKET_URI = f\"gs://{PROJECT_ID}-bucket\"   # GCS bucket path to store pipeline artifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76606592-b7a4-44a1-a80b-5c13c4104c7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "593f1db9-c7d4-42d8-a5b1-2f7724b45572",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6be318a5-cb5c-4332-b6c0-b4a5e1317f2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.dsl import Output, Dataset\n",
    "\n",
    "\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install=[\n",
    "        \"pandas==1.3.4\",\n",
    "        \"scikit-learn==1.0.1\",\n",
    "        \"google-cloud-bigquery==3.13.0\",\n",
    "        \"db-dtypes==1.1.1\",\n",
    "    ],\n",
    ")\n",
    "def get_data(\n",
    "    project_id: str,\n",
    "    dataset_train: Output[Dataset],\n",
    "    dataset_test: Output[Dataset],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Loads data from BigQuery, splits it into training and test sets,\n",
    "    and saves them as CSV files.\n",
    "\n",
    "    Args:\n",
    "        project_id: str, Google Cloud project ID\n",
    "        dataset_train: Output[Dataset] for the training set\n",
    "        dataset_test: Output[Dataset] for the test set\n",
    "    \"\"\"\n",
    "\n",
    "    # Imports inside component so they're installed in the container\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pandas as pd\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    # Construct a BigQuery client object\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "\n",
    "    # BigQuery SQL query\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "      * EXCEPT(fullVisitorId)\n",
    "    FROM\n",
    "    (\n",
    "      SELECT\n",
    "        fullVisitorId,\n",
    "        IFNULL(totals.bounces, 0) AS bounces,\n",
    "        IFNULL(totals.timeOnSite, 0) AS time_on_site\n",
    "      FROM `data-to-insights.ecommerce.web_analytics`\n",
    "      WHERE totals.newVisits = 1\n",
    "        AND date BETWEEN '20160801' AND '20170430'  # train on first 9 months\n",
    "    )\n",
    "    JOIN\n",
    "    (\n",
    "      SELECT\n",
    "        fullVisitorId,\n",
    "        IF(COUNTIF(totals.transactions > 0 AND totals.newVisits IS NULL) > 0, 1, 0) \n",
    "          AS will_buy_on_return_visit\n",
    "      FROM `data-to-insights.ecommerce.web_analytics`\n",
    "      GROUP BY fullVisitorId\n",
    "    )\n",
    "    USING (fullVisitorId)\n",
    "    LIMIT 10000\n",
    "    \"\"\"\n",
    "\n",
    "    # Run query and load results into DataFrame\n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "    df = query_job.to_dataframe()\n",
    "\n",
    "    # Split dataset into train/test sets\n",
    "    train, test = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Save to pipeline outputs\n",
    "    train.to_csv(dataset_train.path, index=False)\n",
    "    test.to_csv(dataset_test.path, index=False)\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2157468e-d129-419a-b2f3-6d09ee76c1df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost==1.6.2 in /opt/conda/lib/python3.10/site-packages (1.6.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from xgboost==1.6.2) (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from xgboost==1.6.2) (1.15.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost==1.6.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94cb3840-d374-45aa-a871-bdc7c6595fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/xgboost/compat.py:105: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from kfp import dsl\n",
    "from kfp.dsl import Input, Output, Dataset, Model\n",
    "\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install=[\n",
    "        \"xgboost==1.6.2\",\n",
    "        \"pandas==1.3.5\",\n",
    "        \"joblib==1.1.0\",\n",
    "        \"scikit-learn==1.0.2\",\n",
    "    ],\n",
    ")\n",
    "def train_model(\n",
    "    dataset: Input[Dataset],\n",
    "    model_artifact: Output[Model],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Trains an XGBoost classifier on a given dataset and saves the model artifact.\n",
    "\n",
    "    Args:\n",
    "        dataset: Input[Dataset]\n",
    "            The training dataset as a Kubeflow component input.\n",
    "        model_artifact: Output[Model]\n",
    "            A Kubeflow component output for saving the trained model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "        This function doesn't return a value; its primary purpose is to produce a model artifact.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    # Load Training Data\n",
    "    data = pd.read_csv(dataset.path)\n",
    "\n",
    "    # Train XGBoost Model\n",
    "    model = XGBClassifier(objective=\"binary:logistic\")\n",
    "    model.fit(\n",
    "        data.drop(columns=[\"will_buy_on_return_visit\"]),\n",
    "        data.will_buy_on_return_visit,\n",
    "    )\n",
    "\n",
    "    # Evaluate and Log Metrics\n",
    "    score = model.score(\n",
    "        data.drop(columns=[\"will_buy_on_return_visit\"]),\n",
    "        data.will_buy_on_return_visit,\n",
    "    )\n",
    "\n",
    "    # Save the Model Artifact\n",
    "    os.makedirs(model_artifact.path, exist_ok=True)\n",
    "    joblib.dump(model, os.path.join(model_artifact.path, \"model.joblib\"))\n",
    "\n",
    "    # Metadata for the Artifact\n",
    "    model_artifact.metadata[\"train_score\"] = float(score)\n",
    "    model_artifact.metadata[\"framework\"] = \"XGBoost\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c1bc6fc-ccda-4ed7-8bbc-92d13b9f486b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (\n",
    "    Input,\n",
    "    Output,\n",
    "    Dataset,\n",
    "    Model,\n",
    "    Metrics,\n",
    "    ClassificationMetrics,\n",
    ")\n",
    "\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install=[\n",
    "        \"xgboost==1.6.2\",\n",
    "        \"pandas==1.3.5\",\n",
    "        \"joblib==1.1.0\",\n",
    "        \"scikit-learn==1.0.2\",\n",
    "        \"google-cloud-storage==2.13.0\",\n",
    "    ],\n",
    ")\n",
    "def eval_model(\n",
    "    test_set: Input[Dataset],\n",
    "    xgb_model: Input[Model],\n",
    "    metrics: Output[ClassificationMetrics],\n",
    "    smetrics: Output[Metrics],\n",
    "    bucket_name: str,\n",
    "    score_threshold: float = 0.8,\n",
    ") -> NamedTuple(\"Outputs\", [(\"deploy\", str)]):\n",
    "    \"\"\"\n",
    "    Evaluates an XGBoost model on a test dataset, logs metrics, and decides whether to deploy.\n",
    "\n",
    "    Args:\n",
    "        test_set: Input[Dataset] - CSV with a target column 'will_buy_on_return_visit'.\n",
    "        xgb_model: Input[Model] - Trained model artifact saved as 'model.joblib' in GCS.\n",
    "        metrics: Output[ClassificationMetrics] - For ROC curve and confusion matrix.\n",
    "        smetrics: Output[Metrics] - For scalar metrics like accuracy.\n",
    "        bucket_name: str - GCS bucket name (no gs:// prefix).\n",
    "        score_threshold: float - minimum accuracy required to deploy.\n",
    "\n",
    "    Returns:\n",
    "        NamedTuple(\"Outputs\", [(\"deploy\", str)]) - \"true\" or \"false\".\n",
    "    \"\"\"\n",
    "    # --- Imports inside component image ---\n",
    "    from google.cloud import storage\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import roc_curve, confusion_matrix\n",
    "\n",
    "    # ---------- 1) Load test data ----------\n",
    "    data = pd.read_csv(test_set.path)\n",
    "    X = data.drop(columns=[\"will_buy_on_return_visit\"])\n",
    "    y = data[\"will_buy_on_return_visit\"]\n",
    "\n",
    "    # ---------- 2) Load trained model from GCS ----------\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    # xgb_model.uri is like: gs://<bucket>/<path-to-artifact-dir>\n",
    "    gs_uri = xgb_model.uri  # e.g., \"gs://my-bucket/pipelines/.../model\"\n",
    "    prefix = f\"gs://{bucket_name}/\"\n",
    "    # Path inside the bucket to the model directory\n",
    "    blob_dir = gs_uri[len(prefix):] if gs_uri.startswith(prefix) else gs_uri\n",
    "    smetrics.log_metric(\"blob_path\", str(blob_dir))\n",
    "\n",
    "    # Model file name created by training step\n",
    "    model_blob = bucket.blob(f\"{blob_dir}/model.joblib\")\n",
    "    with model_blob.open(\"rb\") as f:\n",
    "        model = joblib.load(f)\n",
    "\n",
    "    # ---------- 3) Evaluate ----------\n",
    "    # Probabilities for positive class\n",
    "    y_scores = model.predict_proba(X)[:, 1]\n",
    "    # Class predictions\n",
    "    y_pred = model.predict(X)\n",
    "    # Simple accuracy (demo)\n",
    "    score = model.score(X, y)\n",
    "\n",
    "    # ---------- 4) Log metrics to KFP UI ----------\n",
    "    fpr, tpr, thresholds = roc_curve(y.to_numpy(), y_scores, pos_label=1)\n",
    "    metrics.log_roc_curve(fpr.tolist(), tpr.tolist(), thresholds.tolist())\n",
    "\n",
    "    # Confusion matrix: ensure label order matches categories\n",
    "    labels = [0, 1]\n",
    "    cm = confusion_matrix(y, y_pred, labels=labels)\n",
    "    metrics.log_confusion_matrix([\"False\", \"True\"], cm.tolist())\n",
    "\n",
    "\n",
    "\n",
    "    smetrics.log_metric(\"accuracy\", float(score))\n",
    "\n",
    "    # ---------- 5) Deployment decision ----------\n",
    "    deploy = \"true\" if score >= score_threshold else \"false\"\n",
    "\n",
    "    # ---------- 6) Write back artifact metadata ----------\n",
    "    xgb_model.metadata[\"test_score\"] = float(score)\n",
    "\n",
    "    # ---------- 7) Return flag ----------\n",
    "    Outputs = NamedTuple(\"Outputs\", [(\"deploy\", str)])\n",
    "    return Outputs(deploy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91408e47-4148-424c-96c7-4809b00557a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.dsl import Input, Output, Model, Artifact\n",
    "\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform==1.25.0\"],\n",
    ")\n",
    "def deploy_xgboost_model(\n",
    "    model: Input[Model],\n",
    "    project_id: str,\n",
    "    vertex_endpoint: Output[Artifact],\n",
    "    vertex_model: Output[Model],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Deploys an XGBoost model to Vertex AI Endpoint.\n",
    "\n",
    "    Args:\n",
    "        model: Input[Model]\n",
    "            The trained model to deploy.\n",
    "        project_id: str\n",
    "            The Google Cloud project ID.\n",
    "        vertex_endpoint: Output[Artifact]\n",
    "            Represents the deployed Vertex AI Endpoint.\n",
    "        vertex_model: Output[Model]\n",
    "            Represents the deployed Vertex AI Model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    # --- Initialize Vertex AI with the project ---\n",
    "    aiplatform.init(project=project_id)\n",
    "\n",
    "    # --- Upload the trained model to Vertex AI ---\n",
    "    deployed_model = aiplatform.Model.upload(\n",
    "        display_name=\"xgb-classification\",\n",
    "        artifact_uri=model.uri,\n",
    "        serving_container_image_uri=(\n",
    "            \"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-6:latest\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # --- Deploy model to an endpoint (with machine type) ---\n",
    "    endpoint = deployed_model.deploy(machine_type=\"n1-standard-4\")\n",
    "\n",
    "    # --- Save outputs so other pipeline steps can use them ---\n",
    "    vertex_endpoint.uri = endpoint.resource_name\n",
    "    vertex_model.uri = deployed_model.resource_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27c39a8e-d56e-42f3-8257-1423c615ea86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "BUCKET_NAME = \"gs://\" + PROJECT_ID + \"-bucket\"\n",
    "PIPELINE_ROOT = BUCKET_NAME + \"/pipeline_root/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad20dec9-ed10-41c0-95ad-271879c01c6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    name=\"xgboost-pipeline-with-deployment-v2\",\n",
    ")\n",
    "def pipeline():\n",
    "    \"\"\"\n",
    "    Full XGBoost MLOps pipeline:\n",
    "    1. Load dataset\n",
    "    2. Train model\n",
    "    3. Evaluate model\n",
    "    4. Conditionally deploy model if score >= threshold\n",
    "    \"\"\"\n",
    "\n",
    "    # ----- Step 1: Get Data -----\n",
    "    dataset_op = get_data(project_id=PROJECT_ID)\n",
    "\n",
    "    # ----- Step 2: Train Model -----\n",
    "    training_op = train_model(\n",
    "        dataset=dataset_op.outputs[\"dataset_train\"]\n",
    "    )\n",
    "\n",
    "    # ----- Step 3: Evaluate Model -----\n",
    "    eval_op = eval_model(\n",
    "        test_set=dataset_op.outputs[\"dataset_test\"],\n",
    "        xgb_model=training_op.outputs[\"model_artifact\"],\n",
    "        bucket_name=PROJECT_ID + \"-bucket\",  # GCS bucket name\n",
    "    )\n",
    "\n",
    "    # ----- Step 4: Conditional Deployment -----\n",
    "    with dsl.If(eval_op.outputs[\"deploy\"] == \"true\", name=\"deploy\"):\n",
    "        deploy_op = deploy_xgboost_model(\n",
    "            model=training_op.outputs[\"model_artifact\"],\n",
    "            project_id=PROJECT_ID,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07cdeedb-94b2-4c3d-8323-335e8b283de7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "\n",
    "# Compile the pipeline into a YAML definition file\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=\"pipeline.yaml\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2be024d7-f613-4094-89c2-295ce3af1122",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Create a Vertex AI Pipeline Job\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"demo-pipeline\",      # name youâ€™ll see in the Vertex AI UI\n",
    "    template_path=\"pipeline.yaml\",     # compiled pipeline YAML file\n",
    "    pipeline_root=PIPELINE_ROOT,       # GCS path where pipeline stores artifacts\n",
    ")\n",
    "\n",
    "# Run the job\n",
    "job.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a56487-5435-4f41-a950-8082b36f1fae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m133",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m133"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
